/*! \mainpage Руководство пользователя

__xsDNN C++__ - это разрабатываемая __[shuffle-true](https://github.com/shuffle-true)__ библиотека, которая построена,
 в основном, на __[Eigen](https://eigen.tuxfamily.org/index.php?title=Main_Page)__.
 Она чрезвычайно эффективна для обучения на CPU и поддерживает тонкую настройку многих параметров.

Это руководство разделено на следующие разделы:
- \subpage intro
- \subpage advanced
*/

//---------------------------------------- ----------------------------------

/*! \page intro Введение

 __Обучим простую сеть на MNISTе.__

 # Функция обучения

 ## Загрузка данных

 \code
    Matrix train_image, train_label;

    dataset::parse_mnist_image("../datasets/mnist/train-images-idx3-ubyte",
                               train_image,
                               0,
                               1,
                               0,
                               0);

    dataset::parse_mnist_label("../datasets/mnist/train-labels-idx1-ubyte", train_label);
 \endcode

 ## Построение архитектуры сети

 \code
    NeuralNetwork baseline;

    baseline    << new FullyConnected<init::Normal, activate::ReLU>(784, 128)
                << new FullyConnected<init::Normal, activate::Softmax>(128, 10);

    Output* criterion = new CrossEntropyLoss();
    baseline.set_output(criterion);

    std::vector< std::vector<Scalar> > init_params = {
            {0.0, 1.0 / (784.0 + 128.0)},
            {0.0, 1.0 / (128.0 + 10.0)}
    };

    SGD opt; opt.m_lrate = 0.01;
 \endcode

 ## Обучение и сохранение

 \code
    baseline.fit(opt, train_image, train_label, 16, 5, 42);

    baseline.export_net("example-mnist", "baseline");
 \endcode

 <br>
 <br>
 <br>

 # Функция тестирования

 ## Загрузка данных

 \code
    Matrix test_image, test_label;
    dataset::parse_mnist_image("../datasets/mnist/t10k-images-idx3-ubyte",
                               test_image,
                               0.0,
                               1.0,
                               0.0,
                               0.0);
    dataset::parse_mnist_label("../datasets/mnist/t10k-labels-idx1-ubyte", test_label);
 \endcode

 ## Чтение сети и предсказание

 \code
    NeuralNetwork net;
    net.read_net("example-mnist", "baseline");

    net.eval();
    Matrix predict = net.predict(test_image);
 \endcode

 ## Подсчет метрики

 \code
 void calculate_mseloss(Matrix& label, Matrix& predict)
 {
    const long ncols = predict.cols();
    Matrix model_answer(1, ncols);
    Matrix relabel(1, ncols);

    for (int i = 0; i < ncols; i++)
    {
        Scalar max_elem = predict.col(i).maxCoeff();

        for (int j = 0; j < 10; j++)
        {
            if (predict(j, i) == max_elem)
            {
                model_answer(0, i) = j;
                break;
            }
        }
    }

    for (int i = 0; i < ncols; i++)
    {
        for (int j = 0; j < 10; j++)
        {
            if (label(j, i) == 1)
            {
                relabel(0, i) = j;
                break;
            }
        }
    }

    Scalar error = (relabel - model_answer).squaredNorm() / ncols;
    std::cout << "RMSE Error = " << std::sqrt(error) << std::endl;
 }
 \endcode

 Полный исходный код доступен __[здесь](https://github.com/shuffle-true/dl_new/tree/main/example/mnist)__.

 Теперь вы можете перейти к расширенному разделу \ref advanced.
*/

//---------------------------------------------------------------------------- -------------

/*! \page advanced Расширенное использование
 Эта страница предназначена для опытных пользователей.
 Убедитесь, что вы сначала прочитали \ref intro "введение".

 Прежде всего стоит начать с концептуальных идей использования библиотеки.

 1. _xsDNN_ целиком и полностью написана для обучения на CPU. 100% вычислений (матричные операции и т.д.) полностью векторизованы.

 2. В _xsDNN_ есть 2 основных типа объектов (данных) - __Scalar__ (аналог _double_) и __Matrix__. Подробное описание этих
 типов можно найти в файле __Config__. Все операции с сеткой, например обучение и тестирование, происходят __только__ на этих типах.

 3. _xsDNN_ не имеет сложных зависимостей, но имеет понятную иерархию взаимодействия модулей и различных подпрограмм.
 Относительно сложные для понимания участки кода хорошо задокументированны. Все это позволяет понять не самую простую
 внутреннюю структуру нейросетей. Поэтому данную библиотеку можно использовать, в том числе, для понимания некоторых
 процессов глубинного обучения.

 _xsDNN_ во многом обязана __[этой](https://github.com/tiny-dnn/tiny-dnn)__
 и __[этой](https://github.com/yixuan/MiniDNN)__ библиотекам. Также спасибо __[PyTorch](https://pytorch.org/)__
 и __[TensorFlow](https://www.tensorflow.org/)__.

 После ликбеза, можно приступать к изучению и пониманию _xsDNN_.

 Просто прочитайте информацию о всех модулях _xsDNN_ и все станет понятно. Каждый пример очень подробно проиллюстрирован
 кодом и комментариями.

 - \subpage data
 - \subpage layer
 - \subpage activation
 - \subpage distribution
 - \subpage optimizer
 - \subpage output
 - \subpage net
*/

//---------------------------------------------------------------------------- -------------

/*! \page data Загрузка данных
 *  # О хранении
 *
 *  \note Тип данных __Matrix(D, N)__, где __D__ - размерность признакового описания одного объекта, __N__ - кол-во объектов.
 *
 *  В общем случае любую матрицу, в контексте _xsDNN_, следует рассматривать так:
 *
 *      Столбец - признаки одного объекта.
 *      Строка - один и тот же признак у разных объектов.
 *
 *  # Встроенные и пользовательские данные
 *
 *  В _xsDNN_ реализована подгрузка датасета __MNIST__. Другие сеты будут добавлены в следующих обновлениях.
 *
 *  Для загрузки собственных данных в сеть, необходимо соблюдать 2 условия:
 *
 *  1. Структура данных для хранения - __Matrx__.
 *  2. Тип данных в __Matrix__ - __Scalar__ (double).
 *
 *  Тренировочный и тестовый набор должны соответствовать общему принципу хранения данных в _xsDNN_ (см. Заметку).
 *
 *  Метки классов (целевая переменная) должны соответствовать общему принципу хранения данных в _xsDNN_ (см. Заметку)
 *  и каждому выходному слою, о которых написано ниже.
 *
 *  1. MSELoss - нет ограничений.
 *
 *  2. BinaryEntropyLoss - каждый объект зашифрован либо 0, либо 1.
 *
 *  3. CrossEntropyLoss - каждый объект зашифрован столбцовым вектором (D строк, 1 столбец), который содержит только одну
 *  единицу на объект - под верным классом.
 *
 *  Более подробно можно ознакомиться в соответствующих разделах.
 */

//---------------------------------------------------------------------------- -------------

/*! \page layer Слой
 *
 *  В _xsDNN_ реализовано 3 базовых слоя.
 *
 *  1. Полносвязный (FullyConnected)
 *  2. Dropout
 *  3. Пакетная нормализация (BatchNorm1D)
 *
 *  <br>
 *  <br>
 *  <br>
 *
 *  # Полносвязный
 *
 *  ## Реализовано
 *
 *  1. Инициализация весов / смещений
 *  2. Включение / отключение смещений
 *
 *  <br>
 *  <br>
 *  <br>
 *
 *  # Dropout
 *
 *  ## Реализовано
 *
 *  1. Отключение (обнуление) нейронов с _заданной_ вероятностью.
 *
 *  \note Для генерации маски из распределения Бернулли используется недетерминированный генератор случайных чисел,
 *  т.е. при каждом прямом проходе будет "отключаться" независимая комбинация нейронов.
 *
 *  <br>
 *  <br>
 *  <br>
 *
 *  # Пакетная нормализация
 *
 *  ## Реализовано
 *
 *  1. Нормирование значений нейронов к нулевому срднему и единичной дисперсии.
 *  2. При обратном проходе используется вычислительно усовершенствованный граф вычислений градиентов.
 *  3. Инициализация векторов _gamma & betas_
 *
 *  ## Будет реализовано
 *
 *  1. Аналогичная операция, только для 2D векторов.
 */

//---------------------------------------------------------------------------- -------------

/*! \page activation Активация
 *  Этот раздел в разработке. Обратитесь к странице позже.
 */

//---------------------------------------------------------------------------- -------------

/*! \page distribution Инициализация
 *  Этот раздел в разработке. Обратитесь к странице позже.
 */

//---------------------------------------------------------------------------- -------------

/*! \page optimizer Оптимизация
 *  Этот раздел в разработке. Обратитесь к странице позже.
 */

//---------------------------------------------------------------------------- -------------

/*! \page output Функционал ошибки
 *  Этот раздел в разработке. Обратитесь к странице позже.
 */

//---------------------------------------------------------------------------- -------------

/*! \page net Нейросеть
 *  Этот раздел в разработке. Обратитесь к странице позже.
 */

//---------------------------------------------------------------------------- -------------
