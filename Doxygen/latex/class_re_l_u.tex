\hypertarget{class_re_l_u}{}\doxysection{Класс Re\+LU}
\label{class_re_l_u}\index{ReLU@{ReLU}}


{\ttfamily \#include $<$Re\+LU.\+h$>$}

\doxysubsection*{Открытые статические члены}
\begin{DoxyCompactItemize}
\item 
static void \mbox{\hyperlink{class_re_l_u_a5ee051b38cd1782e463984673bf8c903}{activate}} (const Matrix \&Z, Matrix \&A)
\item 
static void \mbox{\hyperlink{class_re_l_u_ab0d11863877e4e9dd20853e3f0aa14d8}{apply\+\_\+jacobian}} (const Matrix \&Z, const Matrix \&A, const Matrix \&F, Matrix \&G)
\begin{DoxyCompactList}\small\item\em Операция матричного дифференцирования. \end{DoxyCompactList}\item 
static std\+::string \mbox{\hyperlink{class_re_l_u_ad7c87506543101a67310030d74ad367c}{return\+\_\+type}} ()
\end{DoxyCompactItemize}


\doxysubsection{Подробное описание}
Класс функции активации -\/ \mbox{\hyperlink{class_re_l_u}{Re\+LU}}. 

\doxysubsection{Методы}
\mbox{\Hypertarget{class_re_l_u_a5ee051b38cd1782e463984673bf8c903}\label{class_re_l_u_a5ee051b38cd1782e463984673bf8c903}} 
\index{ReLU@{ReLU}!activate@{activate}}
\index{activate@{activate}!ReLU@{ReLU}}
\doxysubsubsection{\texorpdfstring{activate()}{activate()}}
{\footnotesize\ttfamily static void Re\+LU\+::activate (\begin{DoxyParamCaption}\item[{const Matrix \&}]{Z,  }\item[{Matrix \&}]{A }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}, {\ttfamily [static]}}

{\bfseries{Алгоритм}}\+: 
\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{keywordtype}{int} relu\_forward(\textcolor{keyword}{const} Matrix\& Z, Matrix\& A)\{}
\DoxyCodeLine{}
\DoxyCodeLine{    \textcolor{keywordflow}{for} (\textcolor{keywordtype}{int} i = 0; i < Z.size(); i++)}
\DoxyCodeLine{    \{}
\DoxyCodeLine{        A[i] = Z[i] > 0 ? Z[i] : 0;}
\DoxyCodeLine{    \}}
\DoxyCodeLine{}
\DoxyCodeLine{\}}

\end{DoxyCode}
 
\begin{DoxyParams}{Аргументы}
{\em Z} & значения нейронов до активации \\
\hline
{\em A} & значения нейронов после активации \\
\hline
\end{DoxyParams}
\mbox{\Hypertarget{class_re_l_u_ab0d11863877e4e9dd20853e3f0aa14d8}\label{class_re_l_u_ab0d11863877e4e9dd20853e3f0aa14d8}} 
\index{ReLU@{ReLU}!apply\_jacobian@{apply\_jacobian}}
\index{apply\_jacobian@{apply\_jacobian}!ReLU@{ReLU}}
\doxysubsubsection{\texorpdfstring{apply\_jacobian()}{apply\_jacobian()}}
{\footnotesize\ttfamily static void Re\+LU\+::apply\+\_\+jacobian (\begin{DoxyParamCaption}\item[{const Matrix \&}]{Z,  }\item[{const Matrix \&}]{A,  }\item[{const Matrix \&}]{F,  }\item[{Matrix \&}]{G }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}, {\ttfamily [static]}}



Операция матричного дифференцирования. 

{\bfseries{Алгоритм}}\+: 
\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{keywordtype}{int} relu\_backprop(\textcolor{keyword}{const} Matrix\& Z, \textcolor{keyword}{const} Matrix\& A,}
\DoxyCodeLine{    \textcolor{keyword}{const} Matrix\& F, Matrix\& G) \{}
\DoxyCodeLine{}
\DoxyCodeLine{    \textcolor{keywordflow}{for} (\textcolor{keywordtype}{int} i = 0; i < A.size(); i++)}
\DoxyCodeLine{    \{}
\DoxyCodeLine{        G[i] = A[i] > 0 ? F[i] : 0;}
\DoxyCodeLine{    \}}
\DoxyCodeLine{}
\DoxyCodeLine{\}}

\end{DoxyCode}
 
\begin{DoxyParams}{Аргументы}
{\em Z} & нейроны слоя до активации. \\
\hline
{\em A} & нейроны слоя после активации. \\
\hline
{\em F} & нейроны следующего слоя. \\
\hline
{\em G} & значения, которые получаются после backprop. \\
\hline
\end{DoxyParams}
\mbox{\Hypertarget{class_re_l_u_ad7c87506543101a67310030d74ad367c}\label{class_re_l_u_ad7c87506543101a67310030d74ad367c}} 
\index{ReLU@{ReLU}!return\_type@{return\_type}}
\index{return\_type@{return\_type}!ReLU@{ReLU}}
\doxysubsubsection{\texorpdfstring{return\_type()}{return\_type()}}
{\footnotesize\ttfamily static std\+::string Re\+LU\+::return\+\_\+type (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}, {\ttfamily [static]}}

\begin{DoxyReturn}{Возвращает}
Тип активации. 
\end{DoxyReturn}


Объявления и описания членов класса находятся в файле\+:\begin{DoxyCompactItemize}
\item 
/home/shuffle/xs\+DNN/xs\+DNN-\/include/neuralnetwork/\+Activation/Re\+LU.\+h\end{DoxyCompactItemize}
